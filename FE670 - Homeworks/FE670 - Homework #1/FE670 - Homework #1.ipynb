{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FE670 - Assignment #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: Sid Bhatia\n",
    "\n",
    "**Date**: October 5th, 2023\n",
    "\n",
    "**Pledge**: I pledge my honor that I have abided by the Stevens Honor System.\n",
    "\n",
    "**Professor**: Dr. Steve Yang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simulate independent random walk price\n",
    "processes, each representing the price of a company’s stock, over\n",
    "1,000 periods using the recursive formulation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P_i(2) = 1 + 0.007 \\times \\epsilon_i(2)$$\n",
    "$$P_i(3) = (1 + 0.007 \\times \\epsilon_i(3) \\times (1 + 0.007 \\times \\epsilon_i(2)))$$\n",
    "$$\\cdots$$\n",
    "$$P_i(n) = (1 + 0.007 \\times \\epsilon_i(n)) \\times \\cdots \\times (1 + 0.007 \\times \\epsilon_i(3)) \\times (1 + 0.007 \\times \\epsilon_i(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $P_i(n)$ represents the $i$-th stock's price at period $n$, $e_i(n)$ are normally normally distributed random processes, and we assume $P_i(1) = 1$. The level of volatility, $0.007$, is compatible with realistic market values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take two random price datasets: stock1 data.csv and\n",
    "stock2 data.csv. Now we will use ChatGPT3.5 to generate a\n",
    "Python program to test cointegration of these two random stocks.\n",
    "Please first create a ChatGPT account using the following website: https://chat.openai.com/chat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you created the account and try to ask ChatGPT the\n",
    "following question: “Please write a Python code to test cointegration of two stocks using both ADF and Johansen test.”\n",
    "Test the Python code generated by ChatGPT and answer the\n",
    "following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Does the code generated by ChatGPT work with the data\n",
    "you generated?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "ADF Test Results for Stock 1:\n",
      "ADF Statistic: -0.19413459769564456\n",
      "P-Value: 0.9392171559832246\n",
      "Critical Values: {'1%': -3.4434437319767452, '5%': -2.8673146875484368, '10%': -2.569845688481135}\n",
      "\n",
      "ADF Test Results for Stock 2:\n",
      "ADF Statistic: -0.039214857513834016\n",
      "P-Value: 0.9551663766326768\n",
      "Critical Values: {'1%': -3.4436568932270095, '5%': -2.8674084917497074, '10%': -2.5698956806372832}\n",
      "\n",
      "Johansen Cointegration Test Results:\n",
      "Trace Statistic: [4.11817378 0.0287301 ]\n",
      "Critical Values (Trace Statistic 90%): [[13.4294 15.4943 19.9349]\n",
      " [ 2.7055  3.8415  6.6349]]\n",
      "Eigenvalues: [8.11321095e-03 5.72296407e-05]\n",
      "\n",
      "Fail to reject the null hypothesis: No cointegration.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "\n",
    "# Define the stock tickers and the date range\n",
    "ticker1 = \"AAPL\"\n",
    "ticker2 = \"MSFT\"\n",
    "start_date = \"2020-01-01\"\n",
    "end_date = \"2021-12-31\"\n",
    "\n",
    "# Fetch stock price data from Yahoo Finance\n",
    "df1 = yf.download(ticker1, start=start_date, end=end_date)['Adj Close']\n",
    "df2 = yf.download(ticker2, start=start_date, end=end_date)['Adj Close']\n",
    "\n",
    "# Ensure the data is sorted by date\n",
    "df1 = df1.sort_index()\n",
    "df2 = df2.sort_index()\n",
    "\n",
    "# Perform the ADF test for each stock\n",
    "adf_result1 = adfuller(df1)\n",
    "adf_result2 = adfuller(df2)\n",
    "\n",
    "print(\"ADF Test Results for Stock 1:\")\n",
    "print(f\"ADF Statistic: {adf_result1[0]}\")\n",
    "print(f\"P-Value: {adf_result1[1]}\")\n",
    "print(f\"Critical Values: {adf_result1[4]}\")\n",
    "\n",
    "print(\"\\nADF Test Results for Stock 2:\")\n",
    "print(f\"ADF Statistic: {adf_result2[0]}\")\n",
    "print(f\"P-Value: {adf_result2[1]}\")\n",
    "print(f\"Critical Values: {adf_result2[4]}\")\n",
    "\n",
    "# Combine the stock price data into a DataFrame\n",
    "price_df = pd.concat([df1, df2], axis=1)\n",
    "price_df.columns = [ticker1, ticker2]\n",
    "\n",
    "# Perform the Johansen test for cointegration\n",
    "johansen_result = coint_johansen(price_df, det_order=0, k_ar_diff=1)\n",
    "\n",
    "print(\"\\nJohansen Cointegration Test Results:\")\n",
    "print(f\"Trace Statistic: {johansen_result.lr1}\")\n",
    "print(f\"Critical Values (Trace Statistic 90%): {johansen_result.cvt}\")\n",
    "print(f\"Eigenvalues: {johansen_result.eig}\")\n",
    "\n",
    "# Check the cointegration hypothesis based on the Trace Statistic\n",
    "trace_statistic_critical_value = johansen_result.cvt[:, 0]  # 90% confidence\n",
    "if johansen_result.lr1[0] > trace_statistic_critical_value[0]:\n",
    "    print(\"\\nReject the null hypothesis: Cointegration is present.\")\n",
    "else:\n",
    "    print(\"\\nFail to reject the null hypothesis: No cointegration.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code by ChatGPT seems to work decently, using the correct libraries (pandas, yfinance, statsmodels). It formulates the data into pandas DataFrames, extracts the relevant metrics needed from the correct tests for unit root stationarity (ADF) as well as cointegration (Johansen) test. It retrieves the critical values necessary to test the respective null hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ii."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Please provide ways to improve the Python code generated\n",
    "by ChatGPT. Please note we have price data not return\n",
    "data.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT used the price data when conducting the tests, when we normally test for stationarity and cointegration for return data. As such, I would modify the code to test the returns for Apple and Microsoft as opposed to prices, since returns are much more likely to be stationarity than prices.\n",
    "\n",
    "In addition, there is a lack of abstraction with the code that ChatGPT created. I would create functions and document the code, specifying the assumptions made and the actual hypotheses of the tests in my code as opposed to writing it in a script-like fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### iii."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Interpret the ADF test result to see whether the stock1 and\n",
    "stock 2 returns are cointegrated.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "ADF Test Results for Stock 1:\n",
      "ADF Statistic: -7.222297651838865\n",
      "P-Value: 2.0960254760026063e-10\n",
      "Critical Values: {'1%': -3.4436568932270095, '5%': -2.8674084917497074, '10%': -2.5698956806372832}\n",
      "\n",
      "ADF Test Results for Stock 2:\n",
      "ADF Statistic: -6.813505641798653\n",
      "P-Value: 2.0872061315642546e-09\n",
      "Critical Values: {'1%': -3.4436568932270095, '5%': -2.8674084917497074, '10%': -2.5698956806372832}\n",
      "\n",
      "Johansen Cointegration Test Results:\n",
      "Trace Statistic: [453.91966334 205.24544811]\n",
      "Critical Values (Trace Statistic 90%): [[13.4294 15.4943 19.9349]\n",
      " [ 2.7055  3.8415  6.6349]]\n",
      "Eigenvalues: [0.39125494 0.33613174]\n",
      "\n",
      "Reject the null hypothesis: Cointegration is present.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "\n",
    "# Define the stock tickers and the date range\n",
    "ticker1 = \"AAPL\"\n",
    "ticker2 = \"MSFT\"\n",
    "start_date = \"2020-01-01\"\n",
    "end_date = \"2021-12-31\"\n",
    "\n",
    "# Fetch stock price data from Yahoo Finance\n",
    "df1 = yf.download(ticker1, start=start_date, end=end_date)['Adj Close']\n",
    "df2 = yf.download(ticker2, start=start_date, end=end_date)['Adj Close']\n",
    "\n",
    "# Ensure the data is sorted by date\n",
    "df1 = df1.sort_index()\n",
    "df2 = df2.sort_index()\n",
    "\n",
    "# Calculate the returns of the two stocks\n",
    "returns1 = df1.pct_change().dropna()\n",
    "returns2 = df2.pct_change().dropna()\n",
    "\n",
    "# Perform the ADF test for each stock\n",
    "adf_result1 = adfuller(returns1)\n",
    "adf_result2 = adfuller(returns2)\n",
    "\n",
    "print(\"ADF Test Results for Stock 1:\")\n",
    "print(f\"ADF Statistic: {adf_result1[0]}\")\n",
    "print(f\"P-Value: {adf_result1[1]}\")\n",
    "print(f\"Critical Values: {adf_result1[4]}\")\n",
    "\n",
    "print(\"\\nADF Test Results for Stock 2:\")\n",
    "print(f\"ADF Statistic: {adf_result2[0]}\")\n",
    "print(f\"P-Value: {adf_result2[1]}\")\n",
    "print(f\"Critical Values: {adf_result2[4]}\")\n",
    "\n",
    "# Perform the Johansen test for cointegration\n",
    "# Combine the returns of both stocks into a DataFrame\n",
    "returns_df = pd.concat([returns1, returns2], axis=1)\n",
    "\n",
    "# Perform the Johansen cointegration test\n",
    "johansen_result = coint_johansen(returns_df, det_order=0, k_ar_diff=1)\n",
    "\n",
    "print(\"\\nJohansen Cointegration Test Results:\")\n",
    "print(f\"Trace Statistic: {johansen_result.lr1}\")\n",
    "print(f\"Critical Values (Trace Statistic 90%): {johansen_result.cvt}\")\n",
    "print(f\"Eigenvalues: {johansen_result.eig}\")\n",
    "\n",
    "# Check the cointegration hypothesis based on the Trace Statistic\n",
    "trace_statistic_critical_value = johansen_result.cvt[:, 0]  # 90% confidence\n",
    "if johansen_result.lr1[0] > trace_statistic_critical_value[0]:\n",
    "    print(\"\\nReject the null hypothesis: Cointegration is present.\")\n",
    "else:\n",
    "    print(\"\\nFail to reject the null hypothesis: No cointegration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After specifying ChatGPT to use the return data, we see that we reject the null hypothesis that the time series contains a unit root and, thus, is not stationary. As such, both stock 1 and stock 2 are unit root stationary. at the 1% significance level or 99% confidence level since the p-values are less than 0.01 as well as their respective test statistics are less than -3.44."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### iv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Interpret the Johansen test result to see whether the stock1\n",
    "and stock 2 returns are cointegrated.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the results above, given that the critical values for the trace statistic for 90%, we reject the null hypothesis that the there is no cointegration between the two stocks. As such, we accept the alternative hypothesis that cointegration exists between Apple and Microsoft."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### v."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Discuss whether you can trust the results from the (3) and\n",
    "(4) questions, and explain the reason.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe we can trust the results from *iii.* and *iv.* questions due to the fact that these are the correct tests and libraries being used for these questions. In addition, the logic that ChatGPT is correct in establishing the correct hypotheses and methodology for testing whether or not we should reject the null hypothesis or if we fail to reject the null hypothesis.\n",
    "\n",
    "After checking its code with other code examples from documentation online, it seems correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generalized autoregressive conditional heteroscedasticity (GARCH) model of Bollerslev (1986) is an important type\n",
    "of time series model for heteroscedastic data. It explicitly models a time-varying conditional variance as a linear function of\n",
    "past squared residuals and of its past values. The GARCH process has been widely used to model economic and financial timeseries data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many extensions of the simple GARCH model have been developed in the literature. This example illustrates estimation of\n",
    "variants of GARCH models using the AUTOREG and MODEL\n",
    "procedures, which include the GARCH-in-mean (GARCH-M) model, Exponential GARCH (EGARCH) model, Quadratic GARCH\n",
    "(QGARCH) model, the Glosten-Jagannathan-Runkle GARCH\n",
    "(GJR-GARCH) model, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The the general form of GARCH(p,q) model (where p is the\n",
    "order of the GARCH terms $\\sigma^2$ and q is the order of the ARCH terms $\\epsilon^2$), following the notation of the original paper, is given\n",
    "by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$r_t = \\mu + \\epsilon_t$$\n",
    "$$\\epsilon_t \\mid \\Psi_{t-1} = \\mathcal{N}(0, \\sigma^2_{t-1})$$\n",
    "$$\\epsilon_t^2 = \\omega + \\alpha_1\\epsilon_{t-1}^2 + \\cdots + \\alpha_q\\epsilon_{t-q}^2 + \\beta_1\\sigma_{t-1}^2 + \\cdots + \\beta_p\\sigma_{t-p}^2$$\n",
    "$$= \\omega + \\sum_{i=1}^q \\alpha_i\\epsilon_{t-i}^2 + \\sum_{i=1}^p \\beta_i\\sigma_{t-i}^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $r_t$ is the return at time $t$, $\\mu$ is the mean return, $\\sigma_t$ is the standard residual at time $t$. We assume the residual $\\epsilon_t$ follows a normal distribution $\\mathcal{N}(0, \\sigma_t^2)$, $\\omega$, $\\alpha_q$ and $\\beta_p$ are the model parameters to be estimated. $\\sigma_{t-1}^2 is the conditional variance at time $t-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, when testing for heteroskedasticity in econometric\n",
    "models, the best test is the White test. However, when dealing\n",
    "with time series data, this means to test for ARCH and GARCH\n",
    "errors. In practice GARCH(1,1) model is mostly used. In this\n",
    "assignment, you are to develop a GARCH(1,1) model and estimate the parameters with the assistant of ChatGPT.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the general GARCH(p, q) mode, we can derive the GARCH(1,1) model as the follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$r_t = \\mu + \\epsilon_t$$\n",
    "$$\\epsilon_t^2 = \\omega + \\alpha\\epsilon_{t-1}^2 + \\beta\\sigma_{t-1}^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Python “arch” package to fit a volatility model for Apple\n",
    "stock daily returns for the duration from 1/1/2020 to 12/31/2022.\n",
    "You may download the data from Yahoo finance or other sources.\n",
    "Try to ask ChatGPT the following question: “How to build a\n",
    "GARCH(1,1) volatility model for a stock return data?” Please\n",
    "answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### i."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Is the procedure generated by ChatGPT correct? Please\n",
    "focus on the data preparation or pre-processing. Please describe what you have learned from the recommendation from\n",
    "ChatGPT.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatGPT Response:\n",
    "\n",
    "Building a GARCH(1,1) volatility model for stock return data involves several steps, including data preparation, model specification, estimation, and evaluation. Here's a step-by-step guide on how to build a GARCH(1,1) model:\n",
    "\n",
    "Data Collection and Preparation:\n",
    "\n",
    "Obtain historical daily or periodic stock return data for the stock you want to model. Ensure the data is in a time series format, with each observation representing a specific time period (e.g., daily returns over several years).\n",
    "Data Exploration:\n",
    "\n",
    "Plot the stock returns to visualize their behavior over time.\n",
    "Check for any trends or seasonality in the data. You may need to remove these components if present.\n",
    "Stationarity:\n",
    "\n",
    "Ensure that the returns are stationary, meaning that their statistical properties (mean, variance) do not change over time. If not stationary, consider differencing the returns or using other techniques to make them stationary.\n",
    "Autocorrelation and Partial Autocorrelation (ACF and PACF):\n",
    "\n",
    "Examine the ACF and PACF plots of the returns to identify potential lag orders for the GARCH model. You're specifically looking for significant lags that indicate autocorrelation in the squared returns (volatility clustering).\n",
    "Model Specification:\n",
    "\n",
    "A GARCH(1,1) model is specified as follows:\n",
    "Mean Equation: Typically, you can use a simple mean model like ARMA or ARIMA for the mean returns.\n",
    "Variance Equation (GARCH(1,1)):\n",
    "Variance at time t (σ^2_t) = ω + α * ε^2_(t-1) + β * σ^2_(t-1)\n",
    "ω, α, and β are parameters to be estimated.\n",
    "ε^2_(t-1) represents the squared returns at time (t-1).\n",
    "Model Estimation:\n",
    "\n",
    "Use a statistical software or programming language like Python (using packages like Statsmodels, ARCH, or GARCH) to estimate the GARCH(1,1) model parameters (ω, α, and β) based on your stock return data.\n",
    "You can use maximum likelihood estimation (MLE) to estimate these parameters.\n",
    "Model Diagnostics:\n",
    "\n",
    "Check the estimated parameters for statistical significance.\n",
    "Examine the residuals (the squared returns) to ensure they exhibit no autocorrelation and have constant variance.\n",
    "Model Evaluation:\n",
    "\n",
    "Evaluate the goodness of fit using statistical tests, such as the Ljung-Box test for white noise residuals.\n",
    "Calculate and plot the conditional volatility (σ_t) predicted by the GARCH(1,1) model and compare it to the actual volatility in the data.\n",
    "Forecasting:\n",
    "\n",
    "Once you have a well-fitted GARCH(1,1) model, you can use it for forecasting future volatility.\n",
    "Risk Management:\n",
    "\n",
    "The GARCH(1,1) model can be used to estimate and manage risk in your portfolio by calculating value-at-risk (VaR) and conditional value-at-risk (CVaR).\n",
    "Remember that financial time series data can be noisy and challenging to model accurately, so it's essential to continuously monitor and update your model as new data becomes available. Additionally, you can explore more complex models, such as GARCH(p, q), if the GARCH(1,1) model does not adequately capture the behavior of the stock returns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach from ChatGPT is generally correct. It had the correct data pre-processing, and it made sure to check for imperative assumptions such as stationarity. It also incorporated data exploration and advised to look at PACF and "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
